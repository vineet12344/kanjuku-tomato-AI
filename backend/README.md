# ğŸ… Kanjuku Tomato AI â€” Backend

## ğŸ§  Project Overview
**Kanjuku Tomato AI** is a deep learningâ€“powered backend built with **FastAPI** that detects **ripe** and **unripe tomatoes** using a trained **YOLOv8** model.  
It provides two inference APIs:
- `/api/predict` â†’ Returns **JSON detections** + a **base64 encoded annotated image**
- `/api/predict/image` â†’ Returns the **annotated image directly**

This backend is designed for **easy integration** with web dashboards, IoT devices, or farm automation systems.

---

## âš™ï¸ Tech Stack
| Category | Technology |
|-----------|-------------|
| **Language** | Python 3.10+ |
| **Framework** | FastAPI |
| **Deep Learning** | Ultralytics YOLOv8 |
| **Image Handling** | Pillow (PIL), NumPy |
| **Model Backend** | PyTorch / ONNX Runtime |
| **Server** | Uvicorn (ASGI) |
| **Environment Config** | python-dotenv |
| **Testing Tools** | Postman / Thunder Client |
| **Containerization (Optional)** | Docker |

---

## ğŸ—‚ï¸ Folder Structure

```

backend/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ main.py # FastAPI app entrypoint
â”‚ â”œâ”€â”€ core/
â”‚ â”‚ â””â”€â”€ config.py # Environment config loader
â”‚ â”œâ”€â”€ routes/
â”‚ â”‚ â””â”€â”€ predict.py # Defines /api/predict endpoints
â”‚ â”œâ”€â”€ services/
â”‚ â”‚ â””â”€â”€ yolo_service.py # YOLO model loading + inference
â”‚ â”œâ”€â”€ model/
â”‚ â”‚ â””â”€â”€ best.pt # YOLOv8 model weights
â”‚ â””â”€â”€ utils/
â”‚ â””â”€â”€ image_utils.py # Optional helpers (drawing, scaling)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .env
â””â”€â”€ README.md

```

## ğŸ§© Features

- âœ… **YOLOv8 model integration** for object detection.
- ğŸ¨ **Annotated output images** with bounding boxes.
- ğŸŒˆ **Color-coded boxes:**  
  - Green â†’ Ripe  
  - Blue â†’ Unripe  
- ğŸ”  **Adaptive font & box scaling** according to image size.
- âš¡ **Asynchronous inference** for non-blocking performance.
- ğŸ”’ **CORS enabled** for frontend communication.
- ğŸ§ª **Easily testable** via Postman, Thunder Client, or curl.
- ğŸ³ **Docker-ready** for production deployment.

---

## ğŸ§° Installation & Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/<your-username>/kanjuku-tomato-backend.git
cd kanjuku-tomato-backend

2ï¸âƒ£ Create Virtual Environment

python3 -m venv .venv
source .venv/bin/activate       # Linux / macOS
# .venv\Scripts\Activate.ps1    # Windows (PowerShell)

3ï¸âƒ£ Upgrade pip

pip install --upgrade pip

4ï¸âƒ£ Install Dependencies

pip install -r requirements.txt

If you get a â€œno space left on deviceâ€ error:

mkdir -p ~/pip_tmp
export TMPDIR=~/pip_tmp
pip install -r requirements.txt

5ï¸âƒ£ Place Your Model File

Put your trained YOLO model (best.pt) inside:

app/model/best.pt

6ï¸âƒ£ Environment Variables (.env)

Create a .env file in the project root:

MODEL_PATH=app/model/best.pt
PORT=5000

ğŸš€ Running the Backend

Start the server with:

uvicorn app.main:app --host 0.0.0.0 --port 5000 --reload

Server output should show:

Uvicorn running on http://0.0.0.0:5000

Now open your browser:

http://127.0.0.1:5000

You should see:

{"message": "Backend is alive"}




ğŸ“¡ API Endpoints
1ï¸âƒ£ Health Check

GET /

Response:

{"message": "Backend is alive"}

2ï¸âƒ£ Predict (JSON + Base64 Image)

POST /api/predict
Request:

    Type: multipart/form-data

    Field: file â†’ image file (jpg/png)

Response:

{
  "status": "success",
  "detections": [
    {
      "label": "ripe",
      "confidence": 0.92,
      "box": [x1, y1, x2, y2]
    },
    {
      "label": "unripe",
      "confidence": 0.84,
      "box": [x1, y1, x2, y2]
    }
  ],
  "annotated_image": "<base64-encoded-image>"
}

Example curl command:

curl -X POST "http://127.0.0.1:5000/api/predict" \
     -F "file=@/path/to/tomato.jpg" \
     -H "Accept: application/json"

Decode image from response:

curl -s -X POST "http://127.0.0.1:5000/api/predict" -F "file=@tomato.jpg" \
| jq -r '.annotated_image' | base64 --decode > annotated.jpg

3ï¸âƒ£ Predict (Direct Annotated Image)

POST /api/predict/image
Request:

    Type: multipart/form-data

    Field: file â†’ image file

Response:

    Returns a binary JPEG image directly (not JSON).

Example curl command:

curl -X POST "http://127.0.0.1:5000/api/predict/image" \
     -F "file=@/path/to/tomato.jpg" \
     --output annotated.jpg

ğŸ§ª Testing in Postman / Thunder Client
ğŸ”¹ Postman

    Set request type to POST.

    URL: http://127.0.0.1:5000/api/predict or /api/predict/image.

    In the Body tab â†’ choose form-data.

    Add field:

    Key: file
    Type: File
    Value: choose your image

    Click Send.

    For /api/predict â†’ view JSON output.

    For /api/predict/image â†’ click Send and Download to save image.

ğŸ”¹ Thunder Client (VS Code Extension)

    Create new request.

    Method: POST
    URL: http://127.0.0.1:5000/api/predict/image

    Add body â†’ form-data â†’ file

    Upload an image.

    Send request.

    Use â€œDownload Responseâ€ to save the result.

ğŸ§¯ Troubleshooting
Issue	Fix
404 Not Found	Make sure route is /api/predict and server is running on same port
Form data requires "python-multipart"	Install with pip install python-multipart
No space left on device	Use TMPDIR=~/pip_tmp pip install ...
Invalid image	Ensure file is a valid JPG/PNG
Bounding boxes too small	Adjust scaling logic or resolution in YOLO model
Wrong colors	Make sure yolo_service.py defines blue for unripe and green for ripe
ğŸ³ Docker Deployment (Optional)
Build Image

docker build -t tomato-backend:latest .

Run Container

docker run -d -p 80:5000 --name tomato-backend tomato-backend:latest

If model not inside image:

docker run -d -p 80:5000 \
  -v $(pwd)/app/model/best.pt:/app/app/model/best.pt \
  tomato-backend:latest

Then open:
ğŸ‘‰ http://localhost/api/predict
â˜ï¸ EC2 Deployment (Quick Guide)

    Launch EC2 (Ubuntu 22.04) and open ports 22 and 80.

    SSH into instance.

    Install Docker:

sudo apt update && sudo apt install docker.io -y

Clone repo:

git clone <repo-url> && cd kanjuku-tomato-backend

Copy your best.pt file to app/model/.

Build and run:

    sudo docker build -t tomato-ai .
    sudo docker run -d -p 80:5000 tomato-ai

    Visit: http://<EC2-PUBLIC-IP>/

ğŸ”’ Security Notes

    Restrict CORS origins in production (allow_origins=["https://your-frontend.com"]).

    Donâ€™t expose model weights publicly.

    Use HTTPS and API authentication if deployed online.

    For GPU-based inference, use EC2 g4dn or AWS SageMaker.

ğŸ’¡ Future Improvements

    Add Redis caching for recent predictions.

    Build frontend dashboard (React/Next.js + Tailwind).

    Add JWT authentication for API users.

    Integrate S3 / Cloud Storage for saving predictions.

    Implement batch inference support.

ğŸ‘¨â€ğŸ’» Author

Vineet Salve & Sai Vijay Chandan R
Backend Developer | Golang & Python Enthusiast
ğŸ“§ Contact for Collaboration
ğŸ License

This project is licensed under the MIT License.
Youâ€™re free to use, modify, and distribute it with attribution.
â­ If you find this project useful, consider starring the repo!


---
